{
    "componentChunkName": "component---src-templates-blog-post-js",
    "path": "/AI/gradient-descent/",
    "result": {"data":{"site":{"siteMetadata":{"title":"HelloCode"}},"markdownRemark":{"id":"0905d476-3394-57a2-a6b5-38cfd65bbff0","excerpt":"梯度下降是一种优化算法，广泛用于机器学习中，目的是通过迭代调整模型参数，最小化损失函数（即预测误差）。 核心概念 损失函数 衡量模型预测值与真实值（训练集）之间的误差，例如均方误差（MSE…","html":"<p>梯度下降是一种优化算法，广泛用于机器学习中，目的是通过迭代调整模型参数，最小化损失函数（即预测误差）。</p>\n<h2>核心概念</h2>\n<ol>\n<li>损失函数</li>\n</ol>\n<ul>\n<li>衡量模型预测值与真实值（训练集）之间的误差，例如均方误差（MSE）</li>\n<li>目标是找到一组参数，使损失函数值最小</li>\n</ul>\n<ol start=\"2\">\n<li>梯度</li>\n</ol>\n<ul>\n<li>梯度是损失函数对每个参数的偏导数，表示参数变化时损失函数的“斜率”</li>\n<li>梯度方向指向损失增加的方向，因此我们沿<strong>负梯度</strong>方向更新参数</li>\n</ul>\n<ol start=\"3\">\n<li>\n<p>更新规则</p>\n<p>参数 w，b 的更新公式：</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"> $w = w - α\\dfrac{∂}{∂_w}J(w,b)$\n\n $b = b - α\\dfrac{∂}{∂_b}J(w,b)$</code></pre></div>\n<p>其中：</p>\n<ul>\n<li>α 是学习率(learning rate)，控制每次步长大小</li>\n</ul>\n</li>\n<li>\n<p>迭代过程</p>\n</li>\n</ol>\n<ul>\n<li>初始化参数(通常随机)</li>\n<li>计算当前参数下的梯度</li>\n<li>按负梯度方向更新参数</li>\n<li>重复直到损失收敛或达到最大迭代次数</li>\n</ul>\n<h2>关键点</h2>\n<ul>\n<li>学习率 α：\n<ul>\n<li>太小：收敛慢</li>\n<li>太大：可能越过最小点，甚至发散</li>\n</ul>\n</li>\n<li>优点：通用、适合大规模数据集、可扩展到复杂模型(如神经网络)</li>\n<li>挑战：\n<ul>\n<li>可能陷入局部最小值</li>\n<li>需调参（如学习率、batch 大小）</li>\n</ul>\n</li>\n<li>收敛：当梯度接近 0 或损失变化很小时，算法停止</li>\n</ul>","frontmatter":{"title":"Gradient descent","date":"April 25, 2025","description":null}},"previous":{"fields":{"slug":"/AI/linear-regression/"},"frontmatter":{"title":"Linear Regression"}},"next":null},"pageContext":{"id":"0905d476-3394-57a2-a6b5-38cfd65bbff0","previousPostId":"0ae45f8a-4e82-5487-bfdc-32eecdfd68f1","nextPostId":null}},
    "staticQueryHashes": ["2841359383","3257411868"]}