{
    "componentChunkName": "component---src-templates-blog-post-js",
    "path": "/AI/decision-trees/",
    "result": {"data":{"site":{"siteMetadata":{"title":"HelloCode"}},"markdownRemark":{"id":"6aecc322-2da0-552a-9c64-2300bba454fa","excerpt":"What is Decision Tree? 决策树是一种广泛应用于机器学习中的监督学习模型，可用于分类和回归任务。它通过树形结构表示决策过程，通过一系列基于特征的条件判断，将输入数据映射到输出。 由以下核心组件构成： 根节点(Root Node…","html":"<h2>What is Decision Tree?</h2>\n<p>决策树是一种广泛应用于机器学习中的监督学习模型，可用于分类和回归任务。它通过<strong>树形结构</strong>表示决策过程，通过一系列基于特征的条件判断，将输入数据映射到输出。</p>\n<p>由以下核心组件构成：</p>\n<ul>\n<li>根节点(Root Node): 树的起点，包含整个数据集，基于某个特征进行分割</li>\n<li>内部节点(Internal Nodes): 每个节点表示一个特征的条件判断（如”年龄 > 30”）, 根据条件将数据分成子集</li>\n<li>分支(Branches): 表示条件判断的过程，代表数据的不同路径</li>\n<li>叶子节点(Leaf Nodes): 树的终点，表示最终的输出</li>\n</ul>\n<h2>How to build a Decision Tree?</h2>\n<ol>\n<li>选择最优特征进行分割</li>\n</ol>\n<p>决策树通过某种标准（如信息增益，基尼指数或者方差减少）选择一个特征和阈值，将数据集分割成子集。常见分裂标准：</p>\n<ul>\n<li>信息增益(Information gain)：基于熵计算，衡量分割后数据纯度的提升，适用于分类任务。熵越低，数据越纯。</li>\n</ul>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>熵</mtext><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mn>1</mn><mi>n</mi></munderover><msub><mi>p</mi><mi>i</mi></msub><mi>l</mi><mi>o</mi><msub><mi>g</mi><mn>2</mn></msub><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">    熵 = - \\sum_1^n p_i log_2(p_i)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord cjk_fallback\">熵</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.9185em;vertical-align:-1.2671em;\"></span><span class=\"mord\">−</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6514em;\"><span style=\"top:-1.8829em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.2671em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">o</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span></div>\n<p>其中 p_i 是类别 i 的概率。</p>\n<ol start=\"2\">\n<li>递归分割</li>\n</ol>\n<p>对每个子集重复上述过程，选择新的特征进行分割，直到满足停止条件（如最大深度, 最小样本数或纯度达到阈值）。</p>\n<ol start=\"3\">\n<li>生成叶节点</li>\n</ol>\n<p>当无法继续分割时，子集形成叶节点，分类任务中叶节点通常取子集中最常见的类别，回归任务取子集的均值。</p>\n<h2>Pros and cons</h2>\n<p>优点：</p>\n<ol>\n<li>易于理解和可视化: 决策树结果直观，类似于人类的决策过程，易于理解和解释，并且可以可视化，方便进行模型分析和规则提取</li>\n<li>无需大量数据预处理: 对缺失值和离群值有一定容忍度，且无需特征归一化或标准化</li>\n<li>支持多种数据类型: 可处理数值型和类别型特征</li>\n<li>计算效率高: 训练和预测速度快，适合中小型数据集</li>\n</ol>\n<p>缺点：</p>\n<ol>\n<li>过拟合风险：决策树容易生成过于复杂的树，过拟合训练数据，特别是在数据噪声较多时</li>\n<li>对数据变化敏感：小的数据变化可能导致树结构完全不同</li>\n<li>偏向于高基数特征：对于类别较多的特征（如 ID 号）可能过度偏好</li>\n<li>不适合捕捉复杂关系：相比神经网络等模型，决策树难以捕捉特征间的非线形关系</li>\n</ol>\n<h2>Tree ensembles</h2>\n<ol>\n<li>决策树集成</li>\n</ol>\n<p>通过从原始数据集中通过 bagging（放回抽样）来构建多个训练集，然后在每个训练集训练决策树，最终将所有模型结构进行综合（如投票或平均）来做出最终预测。</p>\n<p>构建过程：</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Given training set of size m\n\nFor b = 1 to B:\n    Use sampling with replacement to create a new training set of size m\n    Train a decision tree on the new dataset</code></pre></div>\n<ol start=\"2\">\n<li>随机森林算法</li>\n</ol>\n<p>随机森林在 Bagging 的基础上，引入特征选择的随机性：</p>\n<p>构建决策树过程中：在每个节点分裂时，如果有 n 个特征，随机选择 k(k &#x3C; n) 个特征，然后从这部分特征中选择最佳分割特征。</p>\n<ol start=\"3\">\n<li>XGBoost</li>\n</ol>\n<p>在构建数据集时，以更高的概率选择在之前构建的树集合中表现较差的样本。</p>","frontmatter":{"title":"Decision Trees","date":"July 08, 2025","description":null}},"previous":{"fields":{"slug":"/AI/overfitting/"},"frontmatter":{"title":"Overfitting"}},"next":null},"pageContext":{"id":"6aecc322-2da0-552a-9c64-2300bba454fa","previousPostId":"4fa9b611-2998-58f9-98d5-01648a0d12ce","nextPostId":null}},
    "staticQueryHashes": ["2841359383","3257411868"]}