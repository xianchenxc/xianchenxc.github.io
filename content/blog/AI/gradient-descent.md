---
title: Gradient descent
date: 2025-04-25
---

梯度下降是一种优化算法，广泛用于机器学习中，目的是通过迭代调整模型参数，最小化损失函数（即预测误差）。

## 核心概念

1. 损失函数

- 衡量模型预测值与真实值（训练集）之间的误差，例如均方误差（MSE）
- 目标是找到一组参数，使损失函数值最小

2. 梯度

- 梯度是损失函数对每个参数的偏导数，表示参数变化时损失函数的“斜率”
- 梯度方向指向损失增加的方向，因此我们沿**负梯度**方向更新参数

3. 更新规则

参数 w，b 的更新公式：

$$
w = w - α\dfrac{∂}{∂_w}J(w,b)
$$

$$
b = b - α\dfrac{∂}{∂_b}J(w,b)
$$

    其中：
    - α 是学习率(learning rate)，控制每次步长大小

4. 迭代过程

- 初始化参数(通常随机)
- 计算当前参数下的梯度
- 按负梯度方向更新参数
- 重复直到损失收敛或达到最大迭代次数

## 关键点

- 学习率 α：
  - 太小：收敛慢
  - 太大：可能越过最小点，甚至发散
- 优点：通用、适合大规模数据集、可扩展到复杂模型(如神经网络)
- 挑战：
  - 可能陷入局部最小值
  - 需调参（如学习率、batch 大小）
- 收敛：当梯度接近 0 或损失变化很小时，算法停止
  $$
