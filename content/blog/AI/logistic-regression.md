---
title: Logistic Regression
date: 2025-05-10
---

逻辑回归（Logistic Regression）是监督学习的一种算法，主要用于**分类问题**，特别是**二分类**。它通过将线性回归的输出映射到概率空间，预测某个样本属于某个类别的概率。

## 核心概念

1. 模型原理

- 逻辑回归基于线性回归：

$$
 y = w_0x_1 + w_1x_1 + ... + w_nx_n + b
$$

- 通过**Sigmoid 函数**将线性输出 y 映射到 (0,1) 区间，表示概率：

$$
    {\^y} = \cfrac{1}{1 + e^{-z}}
$$

- 分类：设定阈值（通常是 0.5）:
  - ${\^y} >= 0.5$ -> 预测为正类（1）
  - ${\^y} < 0.5$ -> 预测为负类（0）

2. 损失函数

使用对数损失(Log loss):

$$
  \def\modelF{f_{\vec{w}, b}(\vec{x}^{(i)})}
  L(\modelF,y^{(i)}) = \begin{dcases}-log(\modelF) &\text{if }y^{(i)}=1  \\
                                -log(1- \modelF)&\text{if }y^{(i)}=0 \end{dcases}
$$

- $y_i$ 是真实值，$\^y_i$ 是预测概率
- 目标：通过梯度下降最小化损失

简化：

$$
  L(\modelF,y^{(i)}) = -y^{(i)}log(\modelF) - (1-y^{(i)})log(1-\modelF)
$$

## 应用场景

- 垃圾邮件检测（垃圾/非垃圾）
- 疾病预测（患病/未患病）
- 用户行为预测（点击/不点击广告）
- 情感分析（正面/负面）

## 关键点

1. 与线性回归的区别：

- 线性回归预测连续值，逻辑回归预测概率/类别
- 逻辑回归使用 Sigmoid 函数和对数损失，线性回归使用均方误差

2. 优点

- 简单/可解释性强/计算效率高
- 输出概率，便于决策

3. 局限

- 假设特征与结果有线性关系（复杂关系需特征工程或非线性模型）
- 对多分类需要扩展（如 Softmax）
