---
title: Decision Trees
date: 2025-07-08
---

## What is Decision Tree?

决策树是一种广泛应用于机器学习中的监督学习模型，可用于分类和回归任务。它通过**树形结构**表示决策过程，通过一系列基于特征的条件判断，将输入数据映射到输出。

由以下核心组件构成：

- 根节点(Root Node): 树的起点，包含整个数据集，基于某个特征进行分割
- 内部节点(Internal Nodes): 每个节点表示一个特征的条件判断（如"年龄 > 30"）, 根据条件将数据分成子集
- 分支(Branches): 表示条件判断的过程，代表数据的不同路径
- 叶子节点(Leaf Nodes): 树的终点，表示最终的输出

## How to build a Decision Tree?

1. 选择最优特征进行分割

决策树通过某种标准（如信息增益，基尼指数或者方差减少）选择一个特征和阈值，将数据集分割成子集。常见分裂标准：

- 信息增益(Information gain)：基于熵计算，衡量分割后数据纯度的提升，适用于分类任务。熵越低，数据越纯。

$$
    熵 = - \sum_1^n p_i log_2(p_i)
$$

其中 p_i 是类别 i 的概率。

2. 递归分割

对每个子集重复上述过程，选择新的特征进行分割，直到满足停止条件（如最大深度, 最小样本数或纯度达到阈值）。

3. 生成叶节点

当无法继续分割时，子集形成叶节点，分类任务中叶节点通常取子集中最常见的类别，回归任务取子集的均值。

## Pros and cons

优点：

1. 易于理解和可视化: 决策树结果直观，类似于人类的决策过程，易于理解和解释，并且可以可视化，方便进行模型分析和规则提取
2. 无需大量数据预处理: 对缺失值和离群值有一定容忍度，且无需特征归一化或标准化
3. 支持多种数据类型: 可处理数值型和类别型特征
4. 计算效率高: 训练和预测速度快，适合中小型数据集

缺点：

1. 过拟合风险：决策树容易生成过于复杂的树，过拟合训练数据，特别是在数据噪声较多时
2. 对数据变化敏感：小的数据变化可能导致树结构完全不同
3. 偏向于高基数特征：对于类别较多的特征（如 ID 号）可能过度偏好
4. 不适合捕捉复杂关系：相比神经网络等模型，决策树难以捕捉特征间的非线形关系

## Tree ensembles

1. 决策树集成

通过从原始数据集中通过 bagging（放回抽样）来构建多个训练集，然后在每个训练集训练决策树，最终将所有模型结构进行综合（如投票或平均）来做出最终预测。

构建过程：

```
Given training set of size m

For b = 1 to B:
    Use sampling with replacement to create a new training set of size m
    Train a decision tree on the new dataset
```

2. 随机森林算法

随机森林在 Bagging 的基础上，引入特征选择的随机性：

构建决策树过程中：在每个节点分裂时，如果有 n 个特征，随机选择 k(k < n) 个特征，然后从这部分特征中选择最佳分割特征。

3. XGBoost

在构建数据集时，以更高的概率选择在之前构建的树集合中表现较差的样本。
